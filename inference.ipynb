{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bonbak/anaconda3/envs/ULTRA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"rmanluo/RoG-webqsp\", cache_dir=\"/SSL_NAS/concrete/data/webqsp\")\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
    "\n",
    "data = combined_dataset[0]\n",
    "\n",
    "with open('/home/bonbak/ULTRA/custom/train.txt', 'w') as f:\n",
    "    for triple in data['graph']:\n",
    "        triple_str = '\\t'.join(triple) + '\\n'\n",
    "        f.write(triple_str)\n",
    "\n",
    "relation_list = []\n",
    "for triple in data['graph']:\n",
    "    h,r,t = triple\n",
    "    relation_list.append(r)\n",
    "relation_list = list(set(relation_list))\n",
    "\n",
    "src, tgt = data['a_entity'][0], data['q_entity'][0]\n",
    "\n",
    "with open('/home/bonbak/ULTRA/custom/inference_graph.txt', 'w') as f:\n",
    "    for rel in relation_list:\n",
    "        triple = (src,rel,tgt)\n",
    "        triple_str = '\\t'.join(triple) + '\\n'\n",
    "        f.write(triple_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bonbak/anaconda3/envs/ULTRA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultra import util\n",
    "import yaml\n",
    "import easydict\n",
    "\n",
    "def load_yaml(args):\n",
    "    with open(args.file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        content = content.replace(\"{{ dataset }}\", \"\\\"\"+args.dataset+\"\\\"\")\n",
    "        content = content.replace(\"{{ gpus }}\", \"\\\"\"+str(args.gpus)+\"\\\"\")\n",
    "        content = content.replace(\"{{ ckpt }}\", \"\\\"\"+args.checkpoint+\"\\\"\")\n",
    "        content = content.replace(\"{{ bpe }}\", \"\\\"\"+args.bpe+\"\\\"\")\n",
    "        content = content.replace(\"{{ epochs }}\", \"\\\"\"+args.epochs+\"\\\"\")\n",
    "        config = yaml.safe_load(content)\n",
    "        config = easydict.EasyDict(config)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed=1024\n",
    "    dataset = 'CustomTransductiveDataset'\n",
    "    gpus = 0\n",
    "    checkpoint = '/home/bonbak/ULTRA/ckpts/ultra_50g.pth'\n",
    "    file_path = '/home/bonbak/ULTRA/config/transductive/inference.yaml'\n",
    "    bpe = 'null'\n",
    "    epochs = '0'\n",
    "\n",
    "cfg = load_yaml(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CustomTransductiveDataset dataset\n",
      "#train: 9088, #valid: 335, #test: 335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(args.seed + util.get_rank())\n",
    "\n",
    "task_name = cfg.task[\"name\"]\n",
    "dataset = util.build_dataset(cfg)\n",
    "device = f\"cuda:\"+str(args.gpus)\n",
    "\n",
    "train_data, valid_data, test_data = dataset[0], dataset[1], dataset[2]\n",
    "train_data = train_data.to(device)\n",
    "valid_data = valid_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultra.models import Ultra\n",
    "from torch.utils import data as torch_data\n",
    "from ultra import tasks\n",
    "\n",
    "model = Ultra(\n",
    "    rel_model_cfg=cfg.model.relation_model,\n",
    "    entity_model_cfg=cfg.model.entity_model,\n",
    ")\n",
    "\n",
    "if \"checkpoint\" in cfg and cfg.checkpoint is not None:\n",
    "    state = torch.load(cfg.checkpoint, map_location=\"cpu\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "\n",
    "#model = pyg.compile(model, dynamic=True)\n",
    "model = model.to(device)\n",
    "\n",
    "world_size = util.get_world_size()\n",
    "rank = util.get_rank()\n",
    "\n",
    "test_triplets = torch.cat([test_data.target_edge_index, test_data.target_edge_type.unsqueeze(0)]).t()\n",
    "sampler = torch_data.DistributedSampler(test_triplets, world_size, rank)\n",
    "test_loader = torch_data.DataLoader(test_triplets, test_data.num_edge_types, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_vocab():\n",
    "with open(f'{dataset.raw_dir}/entity.dict', 'r') as f:\n",
    "    entities = f.readlines()\n",
    "\n",
    "entity_dict = {}\n",
    "for res in entities:\n",
    "    id, ent = res[:-1].split('\\t')\n",
    "    entity_dict[int(id)] = ent\n",
    "\n",
    "\n",
    "with open(f'{dataset.raw_dir}/relation.dict', 'r') as f:\n",
    "    relations = f.readlines()\n",
    "\n",
    "relation_dict = {}\n",
    "for res in relations:\n",
    "    id, rel = res[:-1].split('\\t')\n",
    "    relation_dict[int(id)] = rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load rspmm extension. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    triples = batch.unsqueeze(1)\n",
    "    pred = model(train_data, triples)\n",
    "    values, indices = pred.squeeze().topk(k=1)\n",
    "    paths, weights = model.visualize(train_data, batch[indices].unsqueeze(0))\n",
    "\n",
    "    inv_pred = model(train_data, triples[:, :, [1, 0, 2]])\n",
    "    inv_values, inv_indices = inv_pred.squeeze().topk(k=1)\n",
    "    inv_paths, inv_weights = model.visualize(train_data, batch[inv_indices].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 129 560\n",
      "Jaxon Bieber people.person.sibling_s m.0gxnnwp\n",
      "560 464 15\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "464",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m h,t,r \u001b[38;5;241m=\u001b[39m triple\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(h,r,t)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(entity_dict[h], \u001b[43mrelation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m, entity_dict[t])\n",
      "\u001b[0;31mKeyError\u001b[0m: 464"
     ]
    }
   ],
   "source": [
    "num_relation = len(relation_dict)\n",
    "for path in paths:\n",
    "    for triple in path:\n",
    "        h,t,r = triple\n",
    "        h_name, t_name = entity_dict[h], entity_dict[t]\n",
    "        r_name = relation_dict[r % num_relation]\n",
    "        if r >= num_relation:\n",
    "            r_name += \"^(-1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaxon Bieber award.award_honor.award_winner Justin Bieber\n",
      "Justin Bieber people.sibling_relationship.sibling Jaxon Bieber\n"
     ]
    }
   ],
   "source": [
    "result_triple_list = batch[indices].detach().tolist()\n",
    "\n",
    "for triple in result_triple_list:\n",
    "    # dataset.inv_rel\n",
    "    h,t,r = triple\n",
    "    print(entity_dict[h], relation_dict[r], entity_dict[t])\n",
    "\n",
    "result_triple_list = batch[inv_indices].detach().tolist()\n",
    "\n",
    "for triple in result_triple_list:\n",
    "    h,t,r = triple\n",
    "    print(entity_dict[t], relation_dict[r],entity_dict[h])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ULTRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
